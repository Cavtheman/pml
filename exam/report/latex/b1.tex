\subsection{Fitting a GP with Pyro}
All code for this task and the next is contained in the \texttt{function\_fitting.ipynb} notebook. We have implemented the required NUTS sampling, and the scatter plot of $p(\theta | \mathcal{D})$ which can be seen in Figure \ref{fig:p_theta}.
\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{images/p_theta.png}
\setlength{\belowcaptionskip}{-10pt}
\caption{Scatter plot showing 500 sampled from $p(\theta | \mathcal{D})$}
\label{fig:p_theta}
\end{figure}

We used Arviz to gauge the quality of our samples, which gave us the plots that can be seen in Figure \ref{fig:arviz_old}. It also told us that our effective bulk sample size is $249$ and $287$ for the \texttt{lengthscale} and \texttt{variance} parameters, respectively. With such few points in the dataset, having half of the samples be good seems to indicate a relatively high quality. However, the quality of the approximation is obviously not very good, as the number of points is not large enough to sufficiently capture the entire signal of the function. Any human asked to approximate the function given only the five points in $\mathcal{D}$ would come up with something very similar to what we find in our plot of $p(f^*|x^*,\mathcal{D})$ in Figure \ref{fig:f_star}.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\linewidth]{images/arviz_single_0.png}\\
\includegraphics[width=0.75\linewidth]{images/arviz_single_1.png}
\setlength{\belowcaptionskip}{-10pt}
\caption{Arviz plots of the results.}
\label{fig:arviz_old}
\end{figure}

% # Lengthscale
% #      | mcse_mean | mcse_sd | ess_bulk | ess_tail | r_hat
% #  0.7 |  0.013    |  0.009  |  165.0   |  228.0   |  1.01
% #  0.8 |  0.011    |  0.008  |  238.0   |  376.0   |  1.01
% #  0.9 |  0.009    |  0.006  |  367.0   |  426.0   |  1.00

% # Variance
% #      | mcse_mean | mcse_sd | ess_bulk | ess_tail | r_hat
% #  0.7 |  0.332    |  0.235  |  347.0   |  414.0   |  1.01
% #  0.8 |  0.254    |  0.180  |  347.0   |  414.0   |  1.01
% #  0.9 |  0.252    |  0.179  |  351.0   |  404.0   |  1.01

\begin{table}[h]
\subfloat[Lengthscale parameter.]{
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
target\_accept\_prob & mcse\_mean & mcse\_sd & ess\_bulk & ess\_tail & r\_hat \\ \hline
0.7 & 0.013 & 0.009 & 165.0 & 228.0 & 1.01 \\ \hline
0.8 & 0.011 & 0.008 & 238.0 & 376.0 & 1.01 \\ \hline
0.9 & 0.009 & 0.006 & 367.0 & 426.0 & 1.00 \\ \hline
\end{tabular}
}}
\subfloat[Variance parameter.]{
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
target\_accept\_prob & mcse\_mean & mcse\_sd & ess\_bulk & ess\_tail & r\_hat \\ \hline
0.7 & 0.332 & 0.235 & 347.0 & 414.0 & 1.01 \\ \hline
0.8 & 0.254 & 0.180 & 347.0 & 414.0 & 1.01 \\ \hline
0.9 & 0.252 & 0.179 & 351.0 & 404.0 & 1.01 \\ \hline
\end{tabular}
}}
\caption{Arviz results for the lengthscale and variance parameters.}
\label{tab:arviz_new}
\end{table}

We did not see huge difference when adjusting the nunber of warmup samples, but we did see a slight improvement when increasing the target acceptance probability. The results of this can be seen in Table \ref{tab:arviz_new}. The effective bulk sample size and effective tail sample size increased along with the target acceptance probability. The Markov Chain Monte Carlo Standard Error (MCSE) also decreased, which is a good sign. MCSE refers to the variability in the estimates of the steady state probabilities of the chain. A lower MCSE indicates that the chain is more precise and reliable. The r\_hat values are all close to 1, indicating that the chains have converged. 

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{images/f_star.png}
\setlength{\belowcaptionskip}{-10pt}
\caption{Plot visualising $p(f^*|x^*, \mathcal{D})$}
\label{fig:f_star}
\end{figure}
