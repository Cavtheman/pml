{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyro.infer import NUTS\n",
    "from pyro.infer.mcmc import MCMC\n",
    "import arviz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f (x):\n",
    "    return np.sin (20 * x) + 2 * np.cos (14 * x) - 2 * np.sin (6 * x)\n",
    "\n",
    "x = torch.tensor ([-1, -0.5, 0, 0.5, 1])\n",
    "#x = torch.linspace (-1, 1, 20)\n",
    "y = f (x)\n",
    "#x = x + torch.randn (x.shape) * 0.05\n",
    "#print (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x = np.linspace (-1, 1, 100)\n",
    "test_y = f (test_x)\n",
    "plt.scatter (x, y)\n",
    "plt.plot (test_x, test_y)\n",
    "#x = torch.linspace (-1, 1, 10)\n",
    "#y = torch.tensor (f (x))\n",
    "#x = x + torch.randn (x.shape) * 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    x,\n",
    "    y,\n",
    "    plot_observed_data=False,\n",
    "    plot_predictions=False,\n",
    "    n_prior_samples=0,\n",
    "    model=None,\n",
    "    kernel=None,\n",
    "    n_test=500,\n",
    "    ax=None,\n",
    "):\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    if plot_observed_data:\n",
    "        ax.plot(x.numpy(), y.numpy(), \"kx\")\n",
    "    if plot_predictions:\n",
    "        Xtest = torch.linspace(-1, 1, n_test)  # test inputs\n",
    "        # compute predictive mean and variance\n",
    "        with torch.no_grad():\n",
    "            if type(model) == gp.models.VariationalSparseGP:\n",
    "                mean, cov = model(Xtest, full_cov=True)\n",
    "            else:\n",
    "                mean, cov = model(Xtest, full_cov=True, noiseless=False)\n",
    "        sd = cov.diag().sqrt()  # standard deviation at each input point x\n",
    "        ax.plot(Xtest.numpy(), mean.numpy(), \"r\", lw=2)  # plot the mean\n",
    "        ax.fill_between(\n",
    "            Xtest.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "            (mean - 2.0 * sd).numpy(),\n",
    "            (mean + 2.0 * sd).numpy(),\n",
    "            color=\"C0\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "    if n_prior_samples > 0:  # plot samples from the GP prior\n",
    "        Xtest = torch.linspace(-1, 1, n_test)  # test inputs\n",
    "        noise = (\n",
    "            model.noise\n",
    "            if type(model) != gp.models.VariationalSparseGP\n",
    "            else model.likelihood.variance\n",
    "        )\n",
    "        cov = kernel.forward(Xtest) + noise.expand(n_test).diag()\n",
    "        samples = dist.MultivariateNormal(\n",
    "            torch.zeros(n_test), covariance_matrix=cov\n",
    "        ).sample(sample_shape=(n_prior_samples,))\n",
    "        ax.plot(Xtest.numpy(), samples.numpy().T, lw=2, alpha=0.4)\n",
    "\n",
    "    ax.set_xlim(-1.2, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y, plot_observed_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "lengthscale = pyro.nn.PyroSample (dist.LogNormal(-1, 1))\n",
    "variance = pyro.nn.PyroSample (dist.LogNormal (0, 2))\n",
    "kernel = gp.kernels.RBF(\n",
    "    input_dim=1#, variance=variance, lengthscale=lengthscale\n",
    ")\n",
    "kernel.variance = variance\n",
    "kernel.lengthscale = lengthscale\n",
    "#kernel.variance = pyro.nn.PyroSample(dist.Uniform(torch.tensor(0.5), torch.tensor(1.5)))\n",
    "#kernel.lengthscale = pyro.nn.PyroSample(dist.Uniform(torch.tensor(1.0), torch.tensor(3.0)))\n",
    "gpr = gp.models.GPRegression(x, y, kernel, noise=torch.tensor(1e-4))\n",
    "#plot(x, y, model=gpr, kernel=kernel, n_prior_samples=10)\n",
    "#_ = plt.ylim((-8, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GP_model (x, y)\n",
    "nuts_kernel = NUTS(gpr.model, adapt_step_size=True, jit_compile=True, ignore_jit_warnings=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=500, num_chains=2, warmup_steps=100)\n",
    "mcmc.run()\n",
    "samples = mcmc.get_samples()\n",
    "\n",
    "for k, v in samples.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.loglog (samples['kernel.lengthscale'], label='lengthscale')\n",
    "#plt.loglog (samples['kernel.variance'], label='variance')\n",
    "print (samples['kernel.lengthscale'].shape)\n",
    "plt.scatter (samples['kernel.lengthscale'], samples['kernel.variance'], label=\"samples\")\n",
    "plt.xscale ('log')\n",
    "plt.yscale ('log')\n",
    "plt.xlabel ('lengthscale')\n",
    "plt.ylabel ('variance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "x_new = torch.linspace (-1, 1, 100)\n",
    "mean, cov = gpr (x_new, full_cov=True, noiseless=False)\n",
    "sd = cov.diag().sqrt()\n",
    "plt.plot(x_new.detach().numpy(), mean.detach().numpy(), \"r\", lw=2, label=\"Function approximation\")  \n",
    "plt.fill_between(\n",
    "    x_new.numpy(),  \n",
    "    (mean - 2.0 * sd).detach().numpy(),\n",
    "    (mean + 2.0 * sd).detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "plt.xlabel (\"x\")\n",
    "plt.ylabel (\"y\")\n",
    "plt.scatter (x, y, label=\"Samples\")\n",
    "plt.plot (test_x, test_y, label=\"True function\")\n",
    "plt.legend (loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arviz.from_pyro(mcmc)\n",
    "summary = arviz.summary(data)\n",
    "print (summary)\n",
    "\n",
    "arviz.plot_posterior (data, var_names=['kernel.lengthscale', 'kernel.variance'])\n",
    "plt.show()\n",
    "arviz.plot_trace (data, var_names=['kernel.lengthscale', 'kernel.variance'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimization (D, X_size, n_iter=10):\n",
    "    X_new = torch.linspace (-1, 1, X_size)\n",
    "\n",
    "    for k in range (n_iter):\n",
    "        print (\"Iteration: \", k+1)\n",
    "        pyro.clear_param_store()\n",
    "        lengthscale = pyro.nn.PyroSample (dist.LogNormal(-1, 1))\n",
    "        variance = pyro.nn.PyroSample (dist.LogNormal (0, 2))\n",
    "        kernel = gp.kernels.RBF(\n",
    "            input_dim=1#, variance=variance, lengthscale=lengthscale\n",
    "        )\n",
    "        kernel.variance = variance\n",
    "        kernel.lengthscale = lengthscale\n",
    "        \n",
    "        gpr = gp.models.GPRegression(D[0], D[1], kernel, noise=torch.tensor(1e-4))\n",
    "        nuts_kernel = NUTS(gpr.model, adapt_step_size=True, jit_compile=True, ignore_jit_warnings=True)\n",
    "        mcmc = MCMC(nuts_kernel, num_samples=50, num_chains=2, warmup_steps=100)\n",
    "        mcmc.run()\n",
    "        samples = mcmc.get_samples()\n",
    "\n",
    "        mean, cov = gpr (X_new, full_cov=True, noiseless=False)\n",
    "        sd = cov.diag().sqrt() \n",
    "\n",
    "        plt.plot (X_new.detach().numpy(), mean.detach().numpy(), \"r\", lw=2, label=\"Function approximation\")\n",
    "        plt.fill_between(\n",
    "            X_new.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "            (mean - 2.0 * sd).detach().numpy(),\n",
    "            (mean + 2.0 * sd).detach().numpy(),\n",
    "            color=\"C0\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "        plt.scatter (D[0][:-1], D[1][:-1], label=\"Samples\")\n",
    "        plt.scatter (D[0][-1], D[1][-1], label=\"New sample\")\n",
    "        plt.plot (test_x, test_y, label=\"True function\")\n",
    "        plt.xlabel (\"x\")\n",
    "        plt.ylabel (\"y\")\n",
    "        plt.legend (loc=\"lower right\")\n",
    "        plt.savefig (\"bayesian_optimization_\" + str (k) + \".png\")\n",
    "        plt.show ()\n",
    "\n",
    "        x_p = X_new [torch.argmax (sd)]\n",
    "        #x_p = X_new [torch.argmin (mean)]\n",
    "        y_p = f (x_p)\n",
    "        D = torch.cat ((D, torch.stack ((x_p, y_p)).reshape (2, 1)), dim=1)\n",
    "\n",
    "    return D, mean\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "D = torch.stack ((x, y))\n",
    "print (D.size())\n",
    "new_D, mean = bayesian_optimization (D, 200, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "lengthscale = pyro.nn.PyroSample (dist.LogNormal(-1, 1))\n",
    "variance = pyro.nn.PyroSample (dist.LogNormal (0, 2))\n",
    "kernel = gp.kernels.RBF(\n",
    "    input_dim=1#, variance=variance, lengthscale=lengthscale\n",
    ")\n",
    "kernel.variance = variance\n",
    "kernel.lengthscale = lengthscale\n",
    "\n",
    "gpr = gp.models.GPRegression(new_D[0], new_D[1], kernel, noise=torch.tensor(1e-4))\n",
    "nuts_kernel = NUTS(gpr.model, adapt_step_size=True, jit_compile=True, ignore_jit_warnings=True, target_accept_prob=0.8)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=100, num_chains=2, warmup_steps=100)\n",
    "mcmc.run()\n",
    "samples = mcmc.get_samples()\n",
    "\n",
    "mean, cov = gpr (torch.linspace (-1, 1, 200), full_cov=True, noiseless=False)\n",
    "sd = cov.diag().sqrt() \n",
    "\n",
    " \n",
    "print (new_D)\n",
    "print (test_y.shape)\n",
    "print (test_x.shape)\n",
    "plt.plot (test_x, test_y, label=\"True function\")\n",
    "plt.plot (torch.linspace (-1, 1, 200), mean.detach().numpy(), \"r\", lw=2, label=\"Function approximation\")\n",
    "plt.scatter (new_D[0], new_D[1], label=\"Samples\")\n",
    "plt.xlabel (\"x\")\n",
    "plt.ylabel (\"y\")\n",
    "plt.legend (loc=\"upper right\")\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arviz.from_pyro(mcmc)\n",
    "summary = arviz.summary(data)\n",
    "print (summary)\n",
    "\n",
    "arviz.plot_posterior (data, var_names=['kernel.lengthscale', 'kernel.variance'])\n",
    "plt.show()\n",
    "arviz.plot_trace (data, var_names=['kernel.lengthscale', 'kernel.variance'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "name": "function_fitting.ipynb",
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
