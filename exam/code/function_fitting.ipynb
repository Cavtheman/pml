{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyro.infer import NUTS\n",
    "from pyro.infer.mcmc import MCMC\n",
    "import arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f (x):\n",
    "    return np.sin (20 * x) + 2 * np.cos (14 * x) - 2 * np.sin (6 * x)\n",
    "\n",
    "x = torch.tensor ([-1, -0.5, 0, 0.5, 1])\n",
    "#x = torch.linspace (-1, 1, 20)\n",
    "y = f (x)\n",
    "#x = x + torch.randn (x.shape) * 0.05\n",
    "#print (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x = np.linspace (-1, 1, 100)\n",
    "test_y = f (test_x)\n",
    "plt.scatter (x, y)\n",
    "plt.plot (test_x, test_y)\n",
    "#x = torch.linspace (-1, 1, 10)\n",
    "#y = torch.tensor (f (x))\n",
    "#x = x + torch.randn (x.shape) * 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    x,\n",
    "    y,\n",
    "    plot_observed_data=False,\n",
    "    plot_predictions=False,\n",
    "    n_prior_samples=0,\n",
    "    model=None,\n",
    "    kernel=None,\n",
    "    n_test=500,\n",
    "    ax=None,\n",
    "):\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    if plot_observed_data:\n",
    "        ax.plot(x.numpy(), y.numpy(), \"kx\")\n",
    "    if plot_predictions:\n",
    "        Xtest = torch.linspace(-1, 1, n_test)  # test inputs\n",
    "        # compute predictive mean and variance\n",
    "        with torch.no_grad():\n",
    "            if type(model) == gp.models.VariationalSparseGP:\n",
    "                mean, cov = model(Xtest, full_cov=True)\n",
    "            else:\n",
    "                mean, cov = model(Xtest, full_cov=True, noiseless=False)\n",
    "        sd = cov.diag().sqrt()  # standard deviation at each input point x\n",
    "        ax.plot(Xtest.numpy(), mean.numpy(), \"r\", lw=2)  # plot the mean\n",
    "        ax.fill_between(\n",
    "            Xtest.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "            (mean - 2.0 * sd).numpy(),\n",
    "            (mean + 2.0 * sd).numpy(),\n",
    "            color=\"C0\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "    if n_prior_samples > 0:  # plot samples from the GP prior\n",
    "        Xtest = torch.linspace(-1, 1, n_test)  # test inputs\n",
    "        noise = (\n",
    "            model.noise\n",
    "            if type(model) != gp.models.VariationalSparseGP\n",
    "            else model.likelihood.variance\n",
    "        )\n",
    "        cov = kernel.forward(Xtest) + noise.expand(n_test).diag()\n",
    "        samples = dist.MultivariateNormal(\n",
    "            torch.zeros(n_test), covariance_matrix=cov\n",
    "        ).sample(sample_shape=(n_prior_samples,))\n",
    "        ax.plot(Xtest.numpy(), samples.numpy().T, lw=2, alpha=0.4)\n",
    "\n",
    "    ax.set_xlim(-1.2, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y, plot_observed_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthscale = pyro.nn.PyroSample (dist.LogNormal(-1, 1))\n",
    "variance = pyro.nn.PyroSample (dist.LogNormal (0, 2))\n",
    "kernel = gp.kernels.RBF(\n",
    "    input_dim=1#, variance=variance, lengthscale=lengthscale\n",
    ")\n",
    "kernel.variance = variance\n",
    "kernel.lengthscale = lengthscale\n",
    "#kernel.variance = pyro.nn.PyroSample(dist.Uniform(torch.tensor(0.5), torch.tensor(1.5)))\n",
    "#kernel.lengthscale = pyro.nn.PyroSample(dist.Uniform(torch.tensor(1.0), torch.tensor(3.0)))\n",
    "gpr = gp.models.GPRegression(x, y, kernel, noise=torch.tensor(1e-4))\n",
    "#plot(x, y, model=gpr, kernel=kernel, n_prior_samples=10)\n",
    "#_ = plt.ylim((-8, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GP_model (x, y)\n",
    "nuts_kernel = NUTS(gpr.model, adapt_step_size=True, jit_compile=True, ignore_jit_warnings=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=500, num_chains=2, warmup_steps=100)\n",
    "mcmc.run()\n",
    "samples = mcmc.get_samples()\n",
    "\n",
    "for k, v in samples.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog (samples['kernel.lengthscale'], label='lengthscale')\n",
    "plt.loglog (samples['kernel.variance'], label='variance')\n",
    "plt.show()\n",
    "\n",
    "x_new = torch.linspace (-1, 1, 100)\n",
    "mean, cov = gpr (x_new, full_cov=True, noiseless=False)\n",
    "sd = cov.diag().sqrt()  # standard deviation at each input point x\n",
    "plt.plot(x_new.detach().numpy(), mean.detach().numpy(), \"r\", lw=2)  # plot the mean\n",
    "plt.fill_between(\n",
    "    x_new.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "    (mean - 2.0 * sd).detach().numpy(),\n",
    "    (mean + 2.0 * sd).detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "plt.scatter (x, y)\n",
    "plt.plot (test_x, test_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arviz.from_pyro(mcmc)\n",
    "summary = arviz.summary(data)\n",
    "print (summary)\n",
    "\n",
    "arviz.plot_posterior (data, var_names=['kernel.lengthscale', 'kernel.variance'])\n",
    "plt.show()\n",
    "arviz.plot_trace (data, var_names=['kernel.lengthscale', 'kernel.variance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use the code from B.1 to build a Bayesian Optimization loop to find a local\n",
    "# optimum of f. For this, implement Algorithm 1. In each iteration, sample a function\n",
    "# observed on X∗\n",
    "# from the posterior predictive of the GP using the dataset. Then you\n",
    "# locate the minimum of the sampled function on X∗ and evaluate f at this position.\n",
    "# Finally, add the new position/value pair to D. Repeat until a target number of\n",
    "# iterations is reached. For the GP use the same prior parameters as in the previous\n",
    "# task and you should also ensure sufficient convergence of the MCMC chain. For X∗\n",
    "# use 200 evenly spaced points in [−1, 1]\n",
    "# optimum of f. For this, implement Algorithm 1. In each iteration, sample a function\n",
    "\n",
    "def bayesian_optimization (D, X_size, n_iter=10):\n",
    "    X_new = torch.linspace (-1, 1, X_size)\n",
    "\n",
    "    for k in range (n_iter):\n",
    "        print (\"Iteration: \", k)\n",
    "        pyro.clear_param_store()\n",
    "        lengthscale = pyro.nn.PyroSample (dist.LogNormal(-1, 1))\n",
    "        variance = pyro.nn.PyroSample (dist.LogNormal (0, 2))\n",
    "        kernel = gp.kernels.RBF(\n",
    "            input_dim=1#, variance=variance, lengthscale=lengthscale\n",
    "        )\n",
    "        kernel.variance = variance\n",
    "        kernel.lengthscale = lengthscale\n",
    "        \n",
    "        gpr = gp.models.GPRegression(D[0], D[1], kernel, noise=torch.tensor(1e-4))\n",
    "        nuts_kernel = NUTS(gpr.model, adapt_step_size=True, jit_compile=True, ignore_jit_warnings=True)\n",
    "        mcmc = MCMC(nuts_kernel, num_samples=1, num_chains=2, warmup_steps=20)\n",
    "        mcmc.run()\n",
    "        samples = mcmc.get_samples()\n",
    "\n",
    "        mean, cov = gpr (X_new, full_cov=True, noiseless=False)\n",
    "        sd = cov.diag().sqrt() \n",
    "\n",
    "        x_p = X_new [torch.argmin (mean)]\n",
    "        y_p = f (x_p)\n",
    "        D = torch.cat ((D, torch.stack ((x_p, y_p)).reshape (2, 1)), dim=1)\n",
    "\n",
    "        plt.plot (X_new.detach().numpy(), mean.detach().numpy(), \"r\", lw=2)  # plot the mean\n",
    "        plt.fill_between(\n",
    "            X_new.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "            (mean - 2.0 * sd).detach().numpy(),\n",
    "            (mean + 2.0 * sd).detach().numpy(),\n",
    "            color=\"C0\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "        plt.scatter (D[0], D[1])\n",
    "        plt.plot (test_x, test_y)\n",
    "        plt.show ()\n",
    "    return D, mean\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "D = torch.stack ((x, y))\n",
    "print (D.size())\n",
    "new_D, mean = bayesian_optimization (D, 200, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (new_D)\n",
    "print (test_y.shape)\n",
    "print (test_x.shape)\n",
    "plt.plot (test_x, test_y)\n",
    "plt.scatter (new_D[0], new_D[1])\n",
    "plt.show ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "name": "function_fitting.ipynb",
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
