{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import einops as ein\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD # -ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Denoiser (nn.Module):\n",
    "    def __init__(self, T):\n",
    "        super (Denoiser, self).__init__()\n",
    "        self.T = T\n",
    "        # self.layer1 = nn.Sequential (\n",
    "        #     nn.Conv2d (1, 32, 3, padding=1),\n",
    "        #     nn.ReLU (),\n",
    "        #     nn.Conv2d (32, 64, 3, padding=1),\n",
    "        #     nn.ReLU ()\n",
    "        # )\n",
    "        # self.layer2 = nn.Sequential (\n",
    "        #     nn.Conv2d (64, 128, 3, padding=1),\n",
    "        #     nn.ReLU (),\n",
    "        #     nn.Conv2d (128, 1, 3, padding=1),\n",
    "        #     nn.Sigmoid ()\n",
    "        # )\n",
    "        self.layer1 = nn.Sequential (\n",
    "            nn.Conv2d (1, 1, 3, padding=1),\n",
    "            nn.ReLU (),\n",
    "            nn.Conv2d (1, 1, 3, padding=1),\n",
    "            nn.ReLU ()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential (\n",
    "            nn.Conv2d (1, 1, 3, padding=1),\n",
    "            nn.ReLU (),\n",
    "            nn.Conv2d (1, 1, 3, padding=1),\n",
    "            nn.Sigmoid ()\n",
    "        )\n",
    "\n",
    "        self.time_embed = nn.Linear (self.T, 1)\n",
    "\n",
    "    def forward (self, x, t):\n",
    "        phi = torch.stack ([torch.tensor([ torch.sin ((i * np.pi * t_ / self.T)) for i in range (2, 2*self.T+1, 2)]) for t_ in t])\n",
    "        time_embed = self.time_embed(phi)\n",
    "        x = self.layer1 (x) + time_embed[:,:,None,None]\n",
    "        return self.layer2 (x)\n",
    "#        return self.layers (x) + self.time_embed (phi)\n",
    "\n",
    "class Diffusion (nn.Module):\n",
    "    def __init__(self, noise_steps=100, device=\"cpu\", latent_dim=2):\n",
    "        super (Diffusion, self).__init__()\n",
    "        #self.beta = ein.repeat (torch.tensor ([0.05]), \"() -> a\", a=noise_steps)\n",
    "        self.beta = torch.linspace (1e-4, 0.02, noise_steps, device=device)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod (self.alpha, dim=0)\n",
    "        self.T = noise_steps\n",
    "        self.eps_model = Denoiser (noise_steps)\n",
    "\n",
    "    def q (self, x0, t):\n",
    "        temp = ein.rearrange (self.alpha_bar[t], \"b -> b () () ()\")\n",
    "        mean = torch.sqrt (temp) * x0\n",
    "        var = 1 - temp\n",
    "        return mean, var\n",
    "        \n",
    "    def sample_q (self, x0, t, epsilon=None):\n",
    "        if epsilon is None:\n",
    "            epsilon = torch.randn_like (x0)\n",
    "\n",
    "        mean, var = self.q (x0, t)\n",
    "        return mean + torch.sqrt (var) * epsilon\n",
    "\n",
    "    def p_sample (self, xt, t):\n",
    "        eps_theta = self.eps_model (xt, t)\n",
    "        alpha_bar = ein.rearrange (self.alpha_bar[t], \"b -> b () () ()\")\n",
    "        alpha = ein.rearrange (self.alpha[t], \"b -> b () () ()\") \n",
    "        eps_coef = (1 - alpha) / torch.sqrt ((1 - alpha_bar))\n",
    "        mean = (1 / torch.sqrt (alpha)) * (xt - eps_coef * eps_theta) \n",
    "        var = self.beta[t]\n",
    "        epsilon = torch.randn (xt.shape, device=xt.device)\n",
    "        return mean + torch.sqrt (var) * epsilon\n",
    "\n",
    "    def loss (self, x0, noise=None):\n",
    "        t = torch.randint (0, self.T, (x0.size(0),), device=x0.device, dtype=torch.long)\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like (x0)\n",
    "        xt = self.sample_q (x0, t, noise)\n",
    "        eps_theta = self.eps_model (xt, t)\n",
    "        return F.mse_loss (noise, eps_theta)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Diffusion (device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "batch_size = 256\n",
    "# Get train and test data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train (epoch):\n",
    "    model.train ()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate (train_loader):\n",
    "        data = data.to (device)\n",
    "        optimizer.zero_grad ()\n",
    "        loss = model.loss (data)\n",
    "        loss.backward ()\n",
    "        train_loss += loss.item ()\n",
    "        optimizer.step ()\n",
    "    print ('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format (\n",
    "        epoch, batch_idx * len (data), len (train_loader.dataset),\n",
    "        100. * batch_idx / len (train_loader),\n",
    "        loss.item () / len (data)))\n",
    "\n",
    "def test (epoch):\n",
    "    model.train ()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad ():\n",
    "        for i, (data, _) in enumerate (test_loader):\n",
    "            data = data.to (device)\n",
    "            loss = model.loss (data)\n",
    "            test_loss += loss.item ()\n",
    "\n",
    "    test_loss /= len (test_loader.dataset)\n",
    "    print ('====> Test set loss: {:.4f}'.format (test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save (model, \"diffusion.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "name": "diffusion.ipynb",
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
